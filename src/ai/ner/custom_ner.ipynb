{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Donald Trump was President of USA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Donald Trump, USA)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Donald Trump\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " was President of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    USA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"><br>The patient was prescribed Aspirin for their heart condition.<br>The doctor recommended \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ibuprofen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " to alleviate the patient's headache.<br>The patient is suffering from diabetes, and they need to take \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Metformin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " regularly.<br>After the surgery, the patient experienced some post-operative complications, including infection.<br>The patient is currently on a regimen of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lisinopril\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " to manage their high blood pressure.<br>The antibiotic course for treating the bacterial infection should be completed as prescribed.<br>The patient's insulin dosage needs to be adjusted to better control their blood sugar levels.<br>The physician suspects that the patient may have pneumonia and has ordered a chest X-ray.<br>The patient's cholesterol levels are high, and they have been advised to take \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Atorvastatin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".<br>The allergy to penicillin was noted in the patient's medical history.<br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp('''\n",
    "The patient was prescribed Aspirin for their heart condition.\n",
    "The doctor recommended Ibuprofen to alleviate the patient's headache.\n",
    "The patient is suffering from diabetes, and they need to take Metformin regularly.\n",
    "After the surgery, the patient experienced some post-operative complications, including infection.\n",
    "The patient is currently on a regimen of Lisinopril to manage their high blood pressure.\n",
    "The antibiotic course for treating the bacterial infection should be completed as prescribed.\n",
    "The patient's insulin dosage needs to be adjusted to better control their blood sugar levels.\n",
    "The physician suspects that the patient may have pneumonia and has ordered a chest X-ray.\n",
    "The patient's cholesterol levels are high, and they have been advised to take Atorvastatin.\n",
    "The allergy to penicillin was noted in the patient's medical history.\n",
    "''')\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corona2.json\") as med:\n",
    "    data = json.loads(med.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "        {\n",
    "            \"text\": example[\"content\"],\n",
    "            \"entities\": [\n",
    "                (annotation[\"start\"], annotation[\"end\"], annotation[\"tag_name\"].upper())\n",
    "                for annotation in example[\"annotations\"]\n",
    "            ],\n",
    "        }\n",
    "        for example in data[\"examples\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import filter_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:00<00:00, 257.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for training_example in tqdm(training_data):\n",
    "    text = training_example[\"text\"]\n",
    "    labels = training_example[\"entities\"]\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start, end, label in labels:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    doc.set_ents(filtered_ents)\n",
    "    doc_bin.add(doc)\n",
    "doc_bin.to_disk(\"train.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./train.spacy --output ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ner = spacy.load(\"model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_ner('''\n",
    "The patient was prescribed Aspirin for their heart condition.\n",
    "The doctor recommended Ibuprofen to alleviate the patient's headache.\n",
    "The patient is suffering from diabetes, and they need to take Metformin regularly.\n",
    "After the surgery, the patient experienced some post-operative complications, including infection.\n",
    "The patient is currently on a regimen of Lisinopril to manage their high blood pressure.\n",
    "The antibiotic course for treating the bacterial infection should be completed as prescribed.\n",
    "The patient's insulin dosage needs to be adjusted to better control their blood sugar levels.\n",
    "The physician suspects that the patient may have pneumonia and has ordered a chest X-ray.\n",
    "The patient's cholesterol levels are high, and they have been advised to take Atorvastatin.\n",
    "The allergy to penicillin was noted in the patient's medical history.\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"><br>The patient was prescribed Aspirin for their heart condition.<br>The doctor recommended \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ibuprofen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICINE</span>\n",
       "</mark>\n",
       " to alleviate the patient's \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    headache\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICALCONDITION</span>\n",
       "</mark>\n",
       ".<br>The patient is suffering from diabetes, and they need to take \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Metformin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICINE</span>\n",
       "</mark>\n",
       " regularly.<br>After the surgery, the patient experienced some \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    post\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICALCONDITION</span>\n",
       "</mark>\n",
       "-operative complications, including \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    infection\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICALCONDITION</span>\n",
       "</mark>\n",
       ".<br>The patient is currently on a regimen of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lisinopril\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICINE</span>\n",
       "</mark>\n",
       " to manage their high blood pressure.<br>The antibiotic course for treating the bacterial infection should be completed as prescribed.<br>The patient's \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    insulin dosage\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICINE</span>\n",
       "</mark>\n",
       " needs to be adjusted to better control their blood sugar levels.<br>The physician suspects that the patient may have \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    pneumonia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICALCONDITION</span>\n",
       "</mark>\n",
       " and has ordered a chest X-ray.<br>The patient's cholesterol levels are high, and they have been advised to take \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Atorvastatin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICINE</span>\n",
       "</mark>\n",
       ".<br>The allergy to penicillin was noted in the patient's medical history.<br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom NER Model using Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4, 5], [6, 7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Example dataset\n",
    "sentences = [\n",
    "    [\"John\", \"lives\", \"in\", \"Paris\"],\n",
    "    [\"Steve\", \"works\", \"at\", \"Google\"]\n",
    "]\n",
    "\n",
    "tags = [\n",
    "    [\"B-PER\", \"O\", \"O\", \"B-LOC\"],\n",
    "    [\"B-PER\", \"O\", \"O\", \"B-ORG\"]\n",
    "]\n",
    "\n",
    "# Word Tokenizer\n",
    "tokenizer = Tokenizer(lower=False, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "# Convert words to integer sequences\n",
    "X = tokenizer.texts_to_sequences(sentences)\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape () instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m num_tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tag_index)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Convert tags to integer sequences\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m y \u001b[38;5;241m=\u001b[39m [[\u001b[43mtag_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m sentence] \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tags]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# print(y)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Abiz\\Technical\\code\\python\\poc-trial-solution\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform labels to normalized encoding.\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Labels as normalized encodings.\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 132\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# transform of empty array is empty array\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Abiz\\Technical\\code\\python\\poc-trial-solution\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1406\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, dtype, warn)\u001b[0m\n\u001b[0;32m   1395\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1396\u001b[0m             (\n\u001b[0;32m   1397\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1402\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1403\u001b[0m         )\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _asarray_with_order(xp\u001b[38;5;241m.\u001b[39mreshape(y, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)), order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m-> 1406\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1407\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[0;32m   1408\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape () instead."
     ]
    }
   ],
   "source": [
    "# Label Encoder for the tags\n",
    "tag_encoder = LabelEncoder()\n",
    "tag_encoder.fit([tag for sentence in tags for tag in sentence])\n",
    "tag_index = tag_encoder.classes_\n",
    "num_tags = len(tag_index)\n",
    "\n",
    "# Convert tags to integer sequences\n",
    "y = [[tag_encoder.transform(tag) for tag in sentence] for sentence in tags]\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences for equal length\n",
    "max_len = max(len(s) for s in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_len, padding=\"post\")\n",
    "y_padded = pad_sequences(y, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "# Convert y to one-hot encoded format for categorical cross-entropy\n",
    "y_padded = [to_categorical(i, num_classes=num_tags) for i in y_padded]\n",
    "y_padded = np.array(y_padded)\n",
    "\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(f\"Number of Tags: {num_tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom NER model using bert based models\n",
    "\n",
    "Reference: https://medium.com/@pasdan/building-custom-named-entity-recognition-ner-models-transformers-9759f8d547d8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "model_checkpoint = 'bert-base-cased'\n",
    "model_output_checkpoint = 'transformers/bnk_stmt_token_2022'\n",
    "\n",
    "entity_groups = [\n",
    "  'TIME',\n",
    "  'PERIOD',\n",
    "  'TEAM',\n",
    "  'PLAYER',\n",
    "  'POSITION',\n",
    "  'FORMATION',\n",
    "  'EVENT',\n",
    "  'DIRECTION',\n",
    "  'QUANTITY',\n",
    "  'UNITS'\n",
    "]\n",
    "entity_groups = [\n",
    "    \"MODE\",\n",
    "    \"BANK\",\n",
    "    \"VPA\",\n",
    "    \"FLAT\",\n",
    "    \"REFERENCE\",\n",
    "    \"TRANSACTION\",\n",
    "    \"BANK_INT\",\n",
    "]\n",
    "\n",
    "labels = ['O'] + \\\n",
    "  [f'B-{label}' for label in entity_groups] + \\\n",
    "  [f'I-{label}' for label in entity_groups]\n",
    "\n",
    "label2id = { label:i for i, label in enumerate(labels) }\n",
    "id2label = { i:label for i, label in enumerate(labels) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from extr_ds.manager.utils.filesystem import load_document\n",
    "\n",
    "def align_labels(tokenized_inputs, label_list):\n",
    "  labels = []\n",
    "  for word_idx in tokenized_inputs.word_ids(batch_index=0):\n",
    "    label_id = -100\n",
    "    if not word_idx is None:\n",
    "      label =  re.sub(r'^[BI]-(.+)$', r'I-\\g<1>', label_list[word_idx]) \\\n",
    "        if word_idx == previous_word_idx \\\n",
    "        else label_list[word_idx]\n",
    "\n",
    "      label_id = label2id[label]\n",
    "\n",
    "    labels.append(label_id)\n",
    "    previous_word_idx = word_idx\n",
    "\n",
    "  return labels\n",
    "\n",
    "def get_dataset(tokenizer, model):\n",
    "  def tokenize_and_align_labels(record):\n",
    "    tokenized_inputs = tokenizer(\n",
    "      record['tokens'],\n",
    "      truncation=True,\n",
    "      is_split_into_words=True\n",
    "    )\n",
    "  \n",
    "    tokenized_inputs['labels'] = align_labels(\n",
    "      tokenized_inputs,\n",
    "      record['labels']\n",
    "    )\n",
    "  \n",
    "    return tokenized_inputs\n",
    "\n",
    "  file_nm = \"ents-iob.json\"\n",
    "  file_nm = \"D:\\\\Abiz\\\\Technical\\\\code\\\\python\\\\poc-trial-solution\\\\src\\\\account\\\\stmt-iob.json\"\n",
    "  ents_dataset = json.loads(\n",
    "    load_document(file_nm)\n",
    "  )\n",
    "\n",
    "  random.shuffle(ents_dataset)\n",
    "\n",
    "  pivot = int(len(ents_dataset) * .8)\n",
    "  data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer,\n",
    "    return_tensors='tf'\n",
    "  )\n",
    "  \n",
    "  train_dataset = Dataset.from_list(ents_dataset[:pivot])\n",
    "  tf_train_set = model.prepare_tf_dataset(\n",
    "    train_dataset.map(\n",
    "      tokenize_and_align_labels,\n",
    "      batched=False\n",
    "    ),\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "  )\n",
    "\n",
    "  test_dataset = Dataset.from_list(ents_dataset[pivot:])\n",
    "  tf_test_set = model.prepare_tf_dataset(\n",
    "    test_dataset.map(\n",
    "      tokenize_and_align_labels,\n",
    "      batched=False\n",
    "    ),\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "  )\n",
    "\n",
    "  return tf_train_set, tf_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import evaluate\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "seqeval = evaluate.load('seqeval')\n",
    "\n",
    "def compute_metrics(preds):\n",
    "  predictions, actuals = preds\n",
    "  predictions = numpy.argmax(predictions, axis=2)\n",
    "\n",
    "  results = seqeval.compute(\n",
    "    predictions=[\n",
    "      [labels[p] for p, l in zip(prediction, label) if l != -100]\n",
    "      for prediction, label in zip(predictions, actuals)\n",
    "    ],\n",
    "    references=[\n",
    "      [labels[l] for p, l in zip(prediction, label) if l != -100]\n",
    "      for prediction, label in zip(predictions, actuals)\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  return {\n",
    "    key: results[f'overall_{key}']\n",
    "    for key in ['precision', 'recall', 'f1', 'accuracy']\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Abiz\\Technical\\code\\python\\poc-trial-solution\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 1420/1420 [00:00<00:00, 1567.35 examples/s]\n",
      "Map: 100%|██████████| 356/356 [00:00<00:00, 2000.64 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "177/177 [==============================] - 599s 3s/step - loss: 0.4303 - val_loss: 0.0420 - precision: 0.9840 - recall: 0.9898 - f1: 0.9869 - accuracy: 0.9931\n",
      "Epoch 2/15\n",
      "177/177 [==============================] - 684s 4s/step - loss: 0.0411 - val_loss: 0.0189 - precision: 0.9948 - recall: 0.9919 - f1: 0.9934 - accuracy: 0.9961\n",
      "Epoch 3/15\n",
      "177/177 [==============================] - 598s 3s/step - loss: 0.0178 - val_loss: 0.0236 - precision: 0.9927 - recall: 0.9963 - f1: 0.9945 - accuracy: 0.9955\n",
      "Epoch 4/15\n",
      "177/177 [==============================] - 556s 3s/step - loss: 0.0093 - val_loss: 0.0266 - precision: 0.9906 - recall: 0.9978 - f1: 0.9942 - accuracy: 0.9909\n",
      "Epoch 5/15\n",
      "177/177 [==============================] - 536s 3s/step - loss: 0.0143 - val_loss: 0.0167 - precision: 0.9963 - recall: 0.9978 - f1: 0.9971 - accuracy: 0.9974\n",
      "Epoch 6/15\n",
      "177/177 [==============================] - 550s 3s/step - loss: 0.0056 - val_loss: 0.0169 - precision: 0.9956 - recall: 0.9978 - f1: 0.9967 - accuracy: 0.9973\n",
      "Epoch 7/15\n",
      "177/177 [==============================] - 539s 3s/step - loss: 0.0032 - val_loss: 0.0068 - precision: 0.9971 - recall: 0.9978 - f1: 0.9974 - accuracy: 0.9975\n",
      "Epoch 8/15\n",
      "177/177 [==============================] - 544s 3s/step - loss: 0.0028 - val_loss: 0.0241 - precision: 0.9971 - recall: 0.9978 - f1: 0.9974 - accuracy: 0.9975\n",
      "Epoch 9/15\n",
      "177/177 [==============================] - 551s 3s/step - loss: 0.0055 - val_loss: 0.0259 - precision: 0.9963 - recall: 0.9978 - f1: 0.9971 - accuracy: 0.9967\n",
      "Epoch 10/15\n",
      "177/177 [==============================] - 531s 3s/step - loss: 0.0027 - val_loss: 0.0225 - precision: 0.9942 - recall: 0.9971 - f1: 0.9956 - accuracy: 0.9964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x2ab7c979ca0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "import tf_keras\n",
    "from transformers import AutoTokenizer, \\\n",
    "                         TFAutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  model_checkpoint\n",
    ")\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "  model_checkpoint,\n",
    "  num_labels=len(labels),\n",
    "  id2label=id2label,\n",
    "  label2id=label2id\n",
    ")\n",
    "\n",
    "tf_train_set, tf_test_set = get_dataset(tokenizer, model)\n",
    "\n",
    "optimizer = tf_keras.optimizers.Adam(learning_rate=2e-5)\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "callbacks = [\n",
    "  KerasMetricCallback(\n",
    "    metric_fn=compute_metrics,\n",
    "    eval_dataset=\n",
    "    tf_test_set\n",
    "  ),\n",
    "  tf_keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "  x=tf_train_set,\n",
    "  validation_data=tf_test_set,\n",
    "  epochs=epochs,\n",
    "  callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_to_save in [tokenizer, model]:\n",
    "  model_to_save.save_pretrained(model_output_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at transformers/bnk_stmt_token_2022 were not used when initializing TFBertForTokenClassification: ['dropout_417']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at transformers/bnk_stmt_token_2022.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'entity_group': 'MODE', 'score': 0.9994918, 'word': 'INFT', 'start': 4, 'end': 8}, {'entity_group': 'TRANSACTION', 'score': 0.99929965, 'word': 'DI14574246', 'start': 9, 'end': 19}, {'entity_group': 'FLAT', 'score': 0.9981221, 'word': '407', 'start': 20, 'end': 23}], [{'entity_group': 'MODE', 'score': 0.99952734, 'word': 'INFT', 'start': 4, 'end': 8}, {'entity_group': 'REFERENCE', 'score': 0.99970555, 'word': '037554667241', 'start': 9, 'end': 21}], [{'entity_group': 'MODE', 'score': 0.9995543, 'word': 'INFT', 'start': 4, 'end': 8}, {'entity_group': 'REFERENCE', 'score': 0.9996999, 'word': '037554196201', 'start': 9, 'end': 21}, {'entity_group': 'FLAT', 'score': 0.93533444, 'word': '##60', 'start': 23, 'end': 25}, {'entity_group': 'BANK', 'score': 0.99970233, 'word': 'VIVISHTECHNOLO', 'start': 57, 'end': 71}]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    'ner', \n",
    "    model=model_output_checkpoint,\n",
    "    aggregation_strategy='simple'\n",
    ")\n",
    "\n",
    "examples = [\n",
    "  '(6:51 - 1st) (Shotgun) P.Mahomes scrambles right end to LAC 34 for 2 yards (S.Joseph; K.Van Noy). FUMBLES (S.Joseph), and recovers at LAC 34.',\n",
    "]\n",
    "examples = [\n",
    "    \"BIL/INFT/DI14574246/407September202/P VIDYA SAGAR /\",\n",
    "    \"INF/INFT/037554667241/A3062770ecf4cd9ad5c04ef9b6679c8e11/\",\n",
    "    \"INF/INFT/037554196201/A6012769586032fcc6e441818d4d7ef6d4/VIVISHTECHNOLO\"\n",
    "]\n",
    "\n",
    "responses = classifier(examples)\n",
    "print(responses)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
